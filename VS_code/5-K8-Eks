# EKS
* Amazon Elastic kubernetes service is fully managed k8 service.
* Eks can be integrated with Other AWS services such as 
- ELB
- CloudWatch
- AGS
- IAM
- VPC

# EKS Cluster Setup
Prerequisites
•AWS Account With Admin Privileges.
•Instance to manage/access EKS cluster using kubectl
•AWS CLI access to use kubectl utility.

Step By Step
1. Create Dedicated VPC for EKS  cluster.
   - Two subnets (Private)
      - private-subnet-1a
      - private-subnet-1b
   - Two subnets (Public)
      - Public-subnet-1a
      - Public-subnet-1b
   - InternetGateWays
   - NAT gateways
   - RouteTable
2. Create IAM Role For Eks cluster
3. Create EKS cluser
   - name
   - version
   - IMA role (EKS cluster role) # this property can't change once cluster has created
*  - Select VPC
   - Select All subnets
   - Security Groups
*  - cluster Endpoints access # Api  server endpoints
     - Public and Private
*  - configure logging (control Plane Logging)
     - Api server
     - Audit
     - Authenticator
     - controller manager
     - scheduler
*  - select add-ons
     - kube-proxy
     - coreDNS
     - Amazon VPC CNI
*  -  select version
     - kube-proxy
     - coreDNS
     - Amazon VPC CNI
4. create # only controle plane has created
# Adding work nodes
5. Create IAM Role for EKS worker nodes
   - AmazonEKSWorkerNodepolicy
   - AmazonEKS_CNI_Policy
   - AmazonEc2ContainerRegistryReadOnly
   - AmazonEBSCSIDriverPolicy
6. Add compute groups 
* Create Worker Nodes 
  - select AMI type = Amazon linux 2 
  - capacity type = on demand
  - instance type = t3.medium # note max ip counts will be given for every instance type.
  - we can add multiple node groups
  - Disk size = 20 GB
* Node Group scaling configuration.
  - Desire size #set desire no of nodes that the group should launch with initially.
  - Minimum size # set the minimu no of nodes that the group can scale in to.
  - maximum size # set the maximum no of nodes that the group can scale out to.
* Node group update configuration
  - set the maximum number or percentage of unavilable nodes to be tolerated during the node group version update.
* Node group networking configuration
  - subnets 
  - pravite subents select
* SSH key pair
  - All
7. Launch normal machine 
*  create An instance
### Install aws CLI In Linux====
1) Download AWS CLI ZIP
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
2) Download & Install Unzip
    sudo yum install unzip -y
3) Extract Zip 
    unzip awscliv2.zip
4) Install
	sudo ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
5) Verify
  aws --version	
   - IAM Authenricator
   - kubectl 
### Install Kubectl In Linux====
1) Install Download the latest kubectl release with the command:
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
2) Make the kubectl binary executable. 
     chmod +x ./kubectl
3) Move the binary in to your PATH.
      sudo mv ./kubectl /usr/local/bin/kubectl
4) Test to ensure the version you installed is up-to-date:
kubectl version --client

   * aws eks uppdate-kubeconfig --name <ClusterName> --region <RegionName>
   * aws eks uppdate-kubeconfig --name EKS-Demo --region ap-south-1
8. Deploy AWS EBS SCSI Driver Plugin
* kubectl apply -k "git---****"
=======================
# How request goes.
LoadBalancer:LoadBalancerPort
            |
NodeIP : NodePort
            |
ServiceIP(ClusterIP):ServicePort
            |
POD IP : TargetPort

# ClusterAuto Scaler
* cluster is tool that automatically adjusts the size of the kubernetes cluster when one of the following conditions is true.
- there are pods that failed to run in the cluster due to insufficient resources.
- there are nodes  in the cluster that have been underutillized for an extended perioed of the time and their pods can be placed on other existing nodes.


===== Cluster Auto Scaler Deployment in EKS========
1) Create AWS policy with below Actions .
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeLaunchConfigurations",
                "autoscaling:DescribeTags",
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup",
                "ec2:DescribeLaunchTemplateVersions"
            ],
            "Resource": "*",
            "Effect": "Allow"
        }
    ]
}	  

2) Attach policy to IAM Role which is used in EKS Node Group.
	
3) Deploy ClusterAutoScaler using below yml(Make sure u update - --node-group-auto-discovery values with your cluster name under cluster-autoscaler deployment commands).

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.14.7
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/EKS_Demo # Update Your ClusterName here
          env:
          - name: AWS_REGION
            value: ap-south-1 # Update Your Region Name here in which u have EKS Cluster
          volumeMounts:
          - name: ssl-certs
            mountPath: /etc/ssl/certs/ca-certificates.crt
            readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
========= RBAC With EKS IAM========================
As a Admin:
=============

1) Create IAM User With Polociy to List & Read EKS Cluster to get Kube Config File in AWS IAM Console.
2) Edit aws-auth to add userarn to aws-auth config map
  # kubectl edit configmap aws-auth -n kube-system

apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::921483055369:role/EKS_Node_Role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::935840844891:user/Balaji          # Update your user arn here
      username: Balaji                                        # Update your user name.
kind: ConfigMap
metadata:
  creationTimestamp: "2020-10-19T03:35:20Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "792449"
  selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth
  uid: 8135dcd1-90e6-4dfb-872f-636601475aca
-----------------------------------------------------------------------------------------
## Create Role/ClusterRole & RoleBinding & ClusterRoleBinding#####

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: readonly
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","list","create","delete","update"]
- apiGroups: ["apps"]
  resources: ["deployments","replicasets","daemonsets"]
  verbs: ["get","list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: full_access_role_binding
  namespace: default
subjects:
- kind: User
  name: Balaji                           # Map with username
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: readonly
  apiGroup: rbac.authorization.k8s.io

Clinet Side:
===========
1) Install Kubectl & AWS CLI.
2) Configure AWS CLI(With Access Key & Secret Key of IAM User which you created in AWS IAM)
3) Get Kube-config file
   aws eks update-kubeconfig --name <EKSClusteName> --region <regionName>
4) try to access the cluster resources using kubectl
=====================================================
# Their are two sorts of isolatiob for pod
* egress -- Incoming request
* ingress -- outgoing (by default pod is non-isolated for ingress)

# How request goes.
LoadBalancer:LoadBalancerPort
            |
NodeIP : NodePort
            |
ServiceIP(ClusterIP):ServicePort
            |
POD IP : TargetPort

# Note :TO avoid above request ,ingressControl is used.
 # IngressControl 
 # IngressRules
 * All the traffice comes from outside cluster will comes from Loadbalancer,ingressControl forwords the request to pods/service to application , This IngressControl will use the ingressRules to allow the requests.

 # Ingress
 - An API object that manages external access to services in a cluster, typically HTTP.
* Clinet ---> Ingress-managed --->Ingresscontrol --->Routing Rule ---> Service ---> Pods
             Load Balancer

* kubernetes ingress is a resource to add rules for routhing traffic from exteral sources to the services in the kubernetes cluster.

# Kubernetes Ingress :
Kubernetes ingress is a native kubernetes resource where you can have rules to route traffic from an external source to service endpoints residing inside the cluster . It requires an ingress controller for routing the rules specified in the ingress object.

# Kubernetes Ingress controller:
Ingress controller is typucally a proxy service deployed in the cluster , it is nothing but a kubernetes deployment exposed to a services .
- nginx ingress controller
- HAproxy
- contour
- traefik
-----------------------------------------------------------
# RBAC ---> Roll based Access controll

1)Admin/Developers:
* Users that are responsible to do administrative or developmental tasks on the cluster .This includes opertions like upgrading the cluster or creating the resources/workloads(PODs,Deployments/PV,Pvc...etc) on the cluster.

2) End users:
* Users that access the applications deployed on our kubernetes cluster.Access restrictions for these users are managed by the applications themselves.
For Example: a web application running on th kubernetes cluster, will have its own security mechanism in place, to prevent unauthorized access.

3) Application/Bots:
* These is possibility that other applications need access to kubernetes cluster or API, typically to talk to resources or workloads inside the  cluster. kubernetes facilitates this by using service accouts.

RBAC in k8's is based on three key concepts:
1. verbs
  - This is a set of operations that can be executed on resources .
  - create, Read, Update or Delete (CRUD)
2. API Reources
  - The set of API objects available in the cluster are called resources.
  - Example: PODS,Deployments,Services,Nodes,PV etc
3. subjects
  -These are the objects allowd access to the API , based on verbs and Resources.
  - Users,Groups,Processes(A service Account)

Note : keep in mind that once you'll create a binding you'll not able to edit its roleRef value-insted, you'll have to delete a Binding and recreate and again.

# Role :
* Roles contains one or more rules that  represent a set of permissions.
* Roles are namespaced , meaning roles work with in the constrains of a namespace.it would default to the defult namespace if none was specified.
* After creating a Role ,asign it to user or group of users by creating a RoleBinding.

EXample : An example Role in the default namespace that can be used to grant read access to pods.

kind: Role
apiVersion:r a .authorization.k8s.io./v1
metadata:
  namespace:default
  name: pod-reader
rules:
 - apiGroup:[""]
   resources: ["pods"]  # we can add "services","configMaps"
   verbs: ["get","watch","list"]
 - apiGroup:["apps"]   # for deployments
   resources: ["deployments","daemonsets","replicaset"]
   verbs: ["get","watch","list","create","patch","delete"]

rules:
 - apiGroup:["*"]
   resources: ["*"]
   verbs: ["*"]

apiGroups: [""] -- set core API group
resources: ["pods"] -- which resources are allowd for access
["get","watch","list"] -- which actions are allowd over the resources above.

# RoleBinding:
* Role Binding is used for granting premission to subject
* RoleBinding Hold a list of subjects (user ,group or service accounts), and a reference to the role being granted.
* Role and RolesBinding are used in namespaces scoped.
* RoleBinding may refernce any role in the same namespace.
* After you create a binding , you cannot change the role that it refers to.if you do want to change the roleRef for a binding ,you need to remove the binding objects and create a again.

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io./v1
metadata:
   name: read-pods
   namespace: default
subject:
- kind: User
  name: Balaji
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
-----------------
# ClusterRole:
* They are applied to the cluster as a whole.
* clusterRole are not bound to a specific namespace.ClusterRole give access across more then one namespace or all namespace.
* After creating a clusterRole , you assign it to a user by creating a clusterRoleBinding.
* clusterRole are cluster-scoped , you can use clusterRole to control access to different kinds of resources than you can with roles.
# cluster-scoped resources (eg: Nodes,pv,pvc,namespaces)

kind: ClusterRole
apiVersion: rabc.authorization.k8s.io./v1
metadata:
  name: demo-clusterole
rules:
 - apiGroup:[""]
   resources: ["pods"]  # we can add "services","configMaps"
   verbs: ["get","watch","list"]

# ClusterRoleBinding:
* clusterRole and ClysterRoleBinding function like Role and Rolebinding ,except they have wider scope.
* RoleBinding grants permissions with in a specific namespace,where as a clusterBinding grants access cluster-wide and to multiple namespaces.
* ClusterRole Binding  is binding or associating a clusterRole with a subject(users,group or service accounts).

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io./v1
metadata:
   name: demo-clusterRolebinding
subject:
- kind: User
  name: Balaji
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: demo-clusterRole
  apiGroup: rbac.authorization.k8s.io
-------
role vs clusterrole

* A clusterrole look similar to role with the only difference that we have to set ,its kind as clusterrole.
* The difference is that role is used inside of namespace , while clusterrole is cluster-wide permission  without a namespace boundariers for example.
- allow access to a cluster node.
- resources in all namespaces.
- allow access to endpoints like /healthz
----------------------------------------------

# Helm
* Helm is a package nanager for K8 applications.
* It will simplify the process of managing application life cycle (Deploying,Upgrading,Deleting) in k8

# 3 rd Party Applications
* MetricServer
* ClusterAutoScaler
* IngressCpntroller(Nginx Controller)
* Mongodb
* Prometheus
* Grafana
* ElasticSearch
* Kibana

# Helm Concepts
* Helm CLI ---> Its command Line interface to work with Helm charts(Packages) for install or upgrade or uninstall helm charts(Packages) & helm repositories to add or remove helm repositories.

* Helm Charts 
-- chart is basically just a collection of manifest files organized in a specific directory structure that describe a related Kubernetes resource. 
-- There are two main components to a chart:
   templates and values.
   These templates and values go through a template rendering engine, producing a manifest that is easily digestible by Kubernetes.

* Helm Repository

# Helm concepts | Charts
Helm packages are called charts, and they consist of a few YAML configuration files and some templates that are rendered into Kubernetes manifest files. Here is the basic directory structure of a chart:
javawebapp
├── Chart.yaml # Meta data information about chart.
├── charts # define dependent helm chart names
├── templates # Will maintain k8’s manifest files which is required for our application
│ ├── deployment.yaml
│ ├── hpa.yaml
│ ├── ingress.yaml
│ ├── service.yaml
│ ├── serviceaccount.yaml
│ └── values.yaml # Will define default values which will be referred in templates.

* charts/: Manually managed chart dependencies can be placed in this directory, though it is typically better to use requirements.yaml to dynamically link dependencies.
* templates/: This directory contains template files that are combined with configuration values (from values.yaml and the command line) and rendered into Kubernetes manifests. The templates use the Go programming language’s template format.
* Chart.yaml: A YAML file with metadata about the chart, such as chart name and version, maintainer information, a relevant website, and search keywords.
* values.yaml: A YAML file of default configuration values for the chart.